{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgEDT4y0G8JM"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This Google Colab notebook aim to propos a possible solution in order to face POMDP problems. This project relyes on two papers:\n",
        "\n",
        "  1.   [Memory-based Deep Reinforcement Learning for POMDPs](https://arxiv.org/pdf/2102.12344.pdf)\n",
        "  >   Original implementation : https://github.com/LinghengMeng/lstm_td3  \n",
        "  Network architecture described in paper\n",
        "  2.   [Deep Reinforcement Learning in Large Discrete Action Spaces](https://arxiv.org/pdf/1512.07679.pdf)\n",
        "  >   Original implementation : https://github.com/ChangyWen/wolpertinger_ddpg  \n",
        "      [Network architecture](https://intellabs.github.io/coach/components/agents/policy_optimization/wolpertinger.html?highlight=wolpertinger)\n",
        " \n",
        "My implementation take inspiration from the above cited works, proposing a Tensorflow version and a different Replay Buffer construction exploiting tensorflow ragged tensor.  \n",
        "The project has been designed in order to study LSTM-TD3 framework, with the addition to investigate the possibility to extend the field of applicability also for discrete actions spaces, since TD3 is inteded to work only with continuos action spaces. This idea came from the fact that Wolpertinger architecture relyes on Deep Deterministic Policy Gradient (DDPG) for train its policy and since TD3 its based on DDPG too, I thougt that could be interesting test Wolpertinger in this different framework. \n",
        "This integration has been reached but performance are not so satisfying due to difficulty on action representability and exploration method. \n",
        "\n",
        "## Environments used\n",
        "\n",
        "Discrete action space - [Lunar Lander v2]( https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
        "\n",
        "Continue action space - [HalfCheetahBulletEnv-v0](https://github.com/benelot/pybullet-gym)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O6hgldc2gEx"
      },
      "source": [
        "### Required packages\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5B5zcHoQdsB"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential python-dev swig python-pygame\n",
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gdown\n",
        "!pip install toml --quiet\n",
        "!pip install --upgrade tensorflow\n",
        "!pip  install tf-agents --quiet\n",
        "!pip install pyflann-py3 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install Box2D --quiet\n",
        "!pip3 install gym[Box_2D] --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7UlbBYe4QKI"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/benelot/pybullet-gym.git \n",
        "!pip install -e pybullet-gym/.\n",
        "!cp -r ./pybullet-gym/pybulletgym /usr/local/lib/python3.7/dist-packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rz4Jm8T1j2n"
      },
      "outputs": [],
      "source": [
        "# Downloading environment utility modules from Github project folder\n",
        "!mkdir ./environment/\n",
        "!wget \"https://github.com/Smaike94/Deep_Learning_Project/blob/main/environment/action_space_TD3.py?raw=true\" -O ./environment/action_space_TD3.py\n",
        "!wget \"https://github.com/Smaike94/Deep_Learning_Project/blob/main/environment/env_wrapper.py?raw=true\" -O ./environment/env_wrapper.py\n",
        "!wget \"https://github.com/Smaike94/Deep_Learning_Project/blob/main/Utility.py?raw=true\" -O ./Utility.py\n",
        "\n",
        "# Downlaod checkpoints folder from personal google drive, in order to only test the model without performing the training\n",
        "!gdown --folder https://drive.google.com/drive/folders/1SiXR89MvKBoADP9HYKOhJXYgubVa3cCQ?usp=sharing\n",
        "!gdown 1tbeA-NHQ2Q47zojYztutfRrria2U1RTr\n",
        "!gdown 1YWeWY0TXJ3BBZn-1auTK26wsWj8WZVVq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AfXaHaP7EBLU"
      },
      "outputs": [],
      "source": [
        "##Including required modules\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import pybulletgym # To register tasks in PyBulletGym\n",
        "import pybullet_envs  # To register tasks in PyBullet\n",
        "\n",
        "import statistics\n",
        "import os\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import Utility\n",
        "import argparse\n",
        "\n",
        "from environment.action_space_TD3 import Discrete_space\n",
        "from environment.env_wrapper import POMDPWrapper\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al7Jah9RfFj1"
      },
      "source": [
        "# Background\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Partially observable Markovian Decision Process (POMDP)\n",
        "This process is defined by a 6-tuple  **<S, A, R, P, O, Ω>**, where **S** is the state space, **A** is the action space, **P**(*s<sub>t+1</sub>*|*s<sub>t</sub>, a<sub>t</sub>*) is the transition probability and **R**(*s<sub>t+1</sub>, s<sub>t</sub>,a<sub>t</sub>*) is the reward function, **O** is the observation space and **Ω** is the observation model. The first 4-tuple also describes *MDP* process, than instead the remaining 2 are specific to *POMDP* process.\n",
        "Agent at each time step *t* has to choose an action *a<sub>t</sub>* from **A**, and starting from current state *s<sub>t</sub>*, **P** model the transition probability toward next state *s<sub>t+1</sub>*, according to state *s<sub>t</sub>* and action *a<sub>t</sub>*. \n",
        "This transition probability is the same both from *MDP* and *POMDP*, with the difference that in latter case the agent doesn't see all information about state's environment but it receives an observation representing a partial representation of it. So, the agent will receive an observation *o<sub>t+1</sub>* from **O** space, when next state *s<sub>t+1</sub>* is reached according to **Ω**(*o<sub>t+1</sub>*|*s<sub>t+1</sub>*) probability. This probability could also be conditioned w.r.t to action *a<sub>t</sub>*, but if it is included in state feature, like the case for this project, the two conditioned probability are the same.  \n",
        "Model-based approaches requires the knowledge of both **P** and **Ω**, instead for this project has been selected a model-free approach exploiting *Long Short Term Memory* network, that provides an help for estimating the underlying state. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Twin Delayed Deep Deterministic Policy Gradient (TD3)\n",
        "\n",
        "This algorithm has been introduced in order to overcome a common problem faced using Deep Deterministic Policy Gradient (DDPG) that is the overestimation of Q-value function. So three main changes have been introduced:\n",
        "\n",
        "* **Clipped Double-Q Learning**: two critic networks have to be trained, and the minimun between them Q-value is selected in target netwotk for Bellman error loss function. \n",
        "\n",
        "*  **Delayed policy update**: policy and target functions have been updated less frequently than critic Q-value function. This allows the value network to become more stable and reduce errors before it is used to update the policy network.\n",
        "\n",
        "*  **Target policy smoothing**: this trick is introduced in order to smooth target value estimate by adding clipped noise to target policy. This concept relyes on the fact that in desirable situation we want that target value have low variance, meaning that similar actions should have similar value. Unfortunatly this not happens in deterministic policy algorithms. So, it is forced this situation by adding noise to target policy during training, inducing target value to be higher on actions that are more resistent to perturtbation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHW0S9__S2zH"
      },
      "source": [
        "# Proposed approach \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ix5soTkk3Kf"
      },
      "source": [
        "### Network architecture \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The suggested implementation for this problem reported in the first paper exploit a recurrent actor-critic framework that uses LSTM network in both actor and critic. Following images show network architecture.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=14p7JE9H4eYhFk62dV8AFkxdx4EZz17sW\" />\n",
        "\n",
        "> Fig.1 Actor network\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1lLHvSUyDeqbu-rvUEi_BMPiEKHFP5wte\" />\n",
        "\n",
        "> Fig.2 Critic network\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1to1NK1z_ejjw5h_g8F4MTonqqGdOwDkr\" width=\"350\" height=\"300\"/>\n",
        "\n",
        "Inside Fully connected blocks could be present more layers, as also in LSTM blocks in which is more correct to speack of cells rather than layers.\n",
        "\n",
        "*  **H<sub>t,l</sub>** represents the result of concatenation between past \n",
        "observations and actions batches. These are 3-dimensional ragged tensor, with *history_length* as ragged dimension. After concatenation the resulting tensor will vary only its third dimension, that becomes *observation_dimension* plus *action_dimension*.  \n",
        "This tensor contains for each row the sequence of past history of length *l* until corrent observation/action at time *t*.  \n",
        "Value *n_observation* is equal to the number of sampled observations during train, instead it is fixed to 1 during environment interaction.\n",
        "\n",
        "*  **O<sub>t</sub>** and **A<sub>t</sub>** are regular tensor of rank 2 representing observations/actions at time *t*. *n_observation* has the same meaning as before. \n",
        "\n",
        "*  **Q(O<sub>t</sub>,A<sub>t</sub>,H<sub>t,l</sub>)** this is the Q-value function represented by critic deep neural network. Its purpose is to approximate the optimal action-value function **Q<sup>*</sup>**. \n",
        "\n",
        "\n",
        "*  **μ(O<sub>t</sub>,H<sub>t,l</sub>)** this is the policy function represented by actor deep neural network. Its purpose is to approximate the optimal action policy **μ<sup>*</sup>**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Pbu045-355"
      },
      "source": [
        "#### Implementation\n",
        "\n",
        "The class *ActorCriticModel* includes code implementation for both actor and critic network, since the two share an almost equal architecture with two main differences:\n",
        "\n",
        "*   critic's input feature network requires both observations and actions, instead of actor that requires only the first\n",
        "*   activation function of last FC layer for critic is the identity and for actor is the hyperbolic tangent.\n",
        "\n",
        "The method *get_input_emb_layers* has been introduced for adding distinct input embedding layers for observations and actions, both when these are used to feed feature or memory network, for discrete action space case in which the reference architecture suggests to handle input in this way.\n",
        "\n",
        "Each layer has its own specific activation function, instead in order to test different network configuration is possible to add more layers and change their size, varying the number of hidden units. This has been achieved through a configuration file written in **TOML** format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3m_zUof8_G62"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_input_emb_layers(act_dim, obs_dim, kernel_init, input_emb_config, past_act=None):\n",
        "    output_dim_act_emb, output_dim_obs_emb = 0, 0\n",
        "    if past_act is not None:\n",
        "        # Input Embedding for memory network\n",
        "        ragged_input = True\n",
        "        input_act_shape, input_obs_shape = (None, act_dim), (None, obs_dim)\n",
        "    else:\n",
        "        # Input Embedding for features network\n",
        "        ragged_input = None\n",
        "        input_act_shape, input_obs_shape = act_dim, obs_dim\n",
        "\n",
        "    input_actor_embedding_net = tf.keras.Sequential()\n",
        "    input_actor_embedding_net.add(layers.Input(shape=input_act_shape, ragged=ragged_input))\n",
        "    for hid_layer_size in input_emb_config[\"input_emb_act\"]:\n",
        "        input_actor_embedding_net.add(layers.Dense(hid_layer_size, activation=\"relu\",\n",
        "                                                   kernel_initializer=kernel_init))\n",
        "    if input_emb_config[\"input_emb_act\"]:\n",
        "        output_dim_act_emb += input_emb_config[\"input_emb_act\"][-1]\n",
        "\n",
        "    input_observation_embedding_net = tf.keras.Sequential()\n",
        "    input_observation_embedding_net.add(layers.Input(shape=input_obs_shape, ragged=ragged_input))\n",
        "    for hid_layer_size in input_emb_config[\"input_emb_obs\"]:\n",
        "        input_observation_embedding_net.add(layers.Dense(hid_layer_size, activation=\"relu\",\n",
        "                                                         kernel_initializer=kernel_init))\n",
        "    if input_emb_config[\"input_emb_obs\"]:\n",
        "        output_dim_obs_emb += input_emb_config[\"input_emb_obs\"][-1]\n",
        "\n",
        "    if past_act is not None:\n",
        "        if past_act:\n",
        "            # Return both embedding net\n",
        "            dim_output = output_dim_act_emb + output_dim_obs_emb\n",
        "            return input_actor_embedding_net, input_observation_embedding_net, dim_output\n",
        "        else:\n",
        "            # Return only observation net\n",
        "            dim_output = output_dim_obs_emb\n",
        "            return None, input_observation_embedding_net, dim_output\n",
        "    else:\n",
        "        # Return both embedding net for features case\n",
        "        dim_output = output_dim_act_emb + output_dim_obs_emb\n",
        "        return input_actor_embedding_net, input_observation_embedding_net, dim_output\n",
        "\n",
        "\n",
        "class ActorCriticModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Combined actor-critic Model.\n",
        "    The following implementation follow the structure described by this paper \n",
        "    https://arxiv.org/pdf/2102.12344.pdf,with the addition of input embedding \n",
        "    layers for Wolpertinger architecture in case for discrete action space, \n",
        "    applied both at input of memory and features network. For Wolpertinger \n",
        "    implementation, https://intellabs.github.io/coach/components/agents/policy_optimization/wolpertinger.html?highlight=wolpertinger.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim, config_net: dict, act_dim, actions_space_continuous, model_type):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "        self.model_type = model_type\n",
        "        self.action_space_continuous = actions_space_continuous\n",
        "        self.mem_net_config = config_net[\"memory_net\"]\n",
        "        self.fet_net_config = config_net[\"features_net\"]\n",
        "        self.comb_net_config = config_net[\"combination_net\"]\n",
        "        self.hist_past_act = config_net[\"hist_past_act\"]\n",
        "\n",
        "        if self.hist_past_act:\n",
        "            input_dim_mem_net = obs_dim + act_dim\n",
        "        else:\n",
        "            input_dim_mem_net = obs_dim\n",
        "        if self.model_type == \"critic\":\n",
        "            self.output_dim = 1\n",
        "            input_dim_fet_net = obs_dim + act_dim\n",
        "            # I.e. use linear activation function according to documentation\n",
        "            activation_output = None\n",
        "        elif self.model_type == \"actor\":\n",
        "            self.output_dim = act_dim\n",
        "            input_dim_fet_net = obs_dim\n",
        "            activation_output = \"tanh\"\n",
        "\n",
        "        kernel_init = \"glorot_uniform\"\n",
        "\n",
        "        # Memory\n",
        "        # In case of discrete action space both batched observation and action in input of memory network have to\n",
        "        # embedded separately, according to Wolpertinger network architecture.\n",
        "        if not self.action_space_continuous:\n",
        "            input_emb_act_net, \\\n",
        "            input_emb_obs_net, \\\n",
        "            dim_output_emb = get_input_emb_layers(act_dim, obs_dim,\n",
        "                                                  kernel_init,\n",
        "                                                  input_emb_config=self.mem_net_config,\n",
        "                                                  past_act=self.hist_past_act)\n",
        "\n",
        "            self.input_actor_memory_embedding = input_emb_act_net\n",
        "            self.input_observation_memory_embedding = input_emb_obs_net\n",
        "            input_dim_mem_net = dim_output_emb if dim_output_emb != 0 else input_dim_mem_net\n",
        "\n",
        "        # pre-RNN\n",
        "        self.memory_network = tf.keras.Sequential()\n",
        "        self.memory_network.add(layers.Input(shape=(None, input_dim_mem_net), ragged=True))\n",
        "        for hid_layer_size in self.mem_net_config[\"pre_rnn_hid_sizes\"]:\n",
        "            self.memory_network.add(layers.Dense(hid_layer_size, activation=\"relu\",\n",
        "                                                 kernel_initializer=kernel_init))\n",
        "\n",
        "        # RNN\n",
        "        if len(self.mem_net_config[\"rnn_hid_sizes\"]) >= 1:\n",
        "            rnn_cells = []\n",
        "            for hid_layer_size in self.mem_net_config[\"rnn_hid_sizes\"]:\n",
        "                rnn_cells.append(layers.LSTMCell(units=hid_layer_size, kernel_initializer=kernel_init))\n",
        "            self.memory_network.add(layers.RNN(cell=rnn_cells))\n",
        "\n",
        "        # post-RNN\n",
        "        for hid_layer_size in self.mem_net_config[\"post_rnn_hid_sizes\"]:\n",
        "            self.memory_network.add(layers.Dense(hid_layer_size, activation=\"relu\",\n",
        "                                                 kernel_initializer=kernel_init))\n",
        "\n",
        "        # Feature extraction\n",
        "        # According to Wolpertinger architecture observations and actions have to embedded separately.\n",
        "        if not self.action_space_continuous and self.model_type == \"critic\":\n",
        "            input_emb_act_net, \\\n",
        "            input_emb_obs_net, \\\n",
        "            dim_output_emb = get_input_emb_layers(act_dim, obs_dim,\n",
        "                                                  kernel_init,\n",
        "                                                  input_emb_config=self.fet_net_config,\n",
        "                                                  past_act=None)\n",
        "            self.input_actor_features_embedding = input_emb_act_net\n",
        "            self.input_observation_features_embedding = input_emb_obs_net\n",
        "            input_dim_fet_net = dim_output_emb if dim_output_emb != 0 else input_dim_fet_net\n",
        "\n",
        "        self.features_network = tf.keras.Sequential()\n",
        "        self.features_network.add(layers.Input(shape=input_dim_fet_net))\n",
        "        for hid_layer_size in self.fet_net_config[\"fet_ext_hid_sizes\"]:\n",
        "            self.features_network.add(layers.Dense(hid_layer_size, activation=\"relu\",\n",
        "                                                   kernel_initializer=kernel_init))\n",
        "\n",
        "        # Combination of memory and feature extraction\n",
        "\n",
        "        self.combination_network = tf.keras.Sequential()\n",
        "        # Retrieve the name of the most recent block of memory network that has a non-empty value for output units\n",
        "        # and accordingly get value.\n",
        "        layer_name = [layers_name for layers_name, layers_struct in self.mem_net_config.items() if layers_struct]\n",
        "        output_size_fet_net = self.fet_net_config[\"fet_ext_hid_sizes\"][-1] if self.fet_net_config[\n",
        "            \"fet_ext_hid_sizes\"] else input_dim_fet_net\n",
        "        input_dim_comb_net = self.mem_net_config[layer_name[-1]][-1] + output_size_fet_net\n",
        "        self.combination_network.add(layers.Input(shape=input_dim_comb_net))\n",
        "\n",
        "        for hid_layer_size in self.comb_net_config[\"comb_net_hid_sizes\"]:\n",
        "            self.combination_network.add(layers.Dense(hid_layer_size, activation=\"relu\",\n",
        "                                                      kernel_initializer=kernel_init))\n",
        "\n",
        "        self.combination_network.add(layers.Dense(self.output_dim, activation=activation_output,\n",
        "                                                  kernel_initializer=kernel_init))\n",
        "\n",
        "    def call(self, inputs: dict) -> tf.Tensor:\n",
        "        memory = inputs[\"memory\"]\n",
        "        features = inputs[\"features\"]\n",
        "        hist_obs = memory[\"obs\"]\n",
        "        hist_act = memory[\"act\"]\n",
        "\n",
        "        obs = features[\"obs\"]\n",
        "        act = features[\"act\"]\n",
        "\n",
        "        if self.action_space_continuous:\n",
        "            if self.hist_past_act:\n",
        "                memory_inputs = tf.concat([hist_obs, hist_act], -1)\n",
        "            else:\n",
        "                memory_inputs = hist_obs\n",
        "        else:\n",
        "            if self.hist_past_act:\n",
        "                out_emb_act_mem = self.input_actor_memory_embedding(hist_act)\n",
        "                out_emb_obs_mem = self.input_observation_memory_embedding(hist_obs)\n",
        "                memory_inputs = tf.concat([out_emb_obs_mem, out_emb_act_mem], -1)\n",
        "            else:\n",
        "                memory_inputs = self.input_observation_memory_embedding(hist_obs)\n",
        "\n",
        "        # Feed memory net\n",
        "        memory_outputs = self.memory_network(memory_inputs)\n",
        "\n",
        "        # Feed extraction net\n",
        "\n",
        "        if self.model_type == \"critic\":\n",
        "            if self.action_space_continuous:\n",
        "                features_inputs = tf.concat([obs, act], -1)\n",
        "            else:\n",
        "                out_emb_act_fet = self.input_actor_features_embedding(act)\n",
        "                out_emb_obs_fet = self.input_observation_features_embedding(obs)\n",
        "                features_inputs = tf.concat([out_emb_obs_fet, out_emb_act_fet], -1)\n",
        "        elif self.model_type == \"actor\":\n",
        "            features_inputs = obs\n",
        "\n",
        "        features_outputs = self.features_network(features_inputs)\n",
        "\n",
        "        # Post-combination\n",
        "        comb_inputs = tf.concat([memory_outputs, features_outputs], -1)\n",
        "        model_output = self.combination_network(comb_inputs)\n",
        "\n",
        "        return model_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6wkvpfKKknM"
      },
      "source": [
        "### Agent \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The *ActorCriticAgent* class represents the entity in algorithm that interacts with the environment. So, it has been structured in order to include instanstations of both actor and critics network. Two types of agent are possible: *main* and *target*. This reflects the different moment in which networks have been applied during the algorithm: *main* agent produce an action that will be used to interact with environment, instead *target* will produce actions that will be used during update for constructing Bellman lookup target. Both use critic networks during update phase.\n",
        "\n",
        "The *get_action* method decides what type of action has to be performed based on the topology of action space.  \n",
        "\n",
        "\n",
        "Standard deviation is not equal for the two agent type, instead network architecture along with action space parameters are the same. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAQVU8PDohai"
      },
      "source": [
        "#### Action space\n",
        "\n",
        "Definitions shared by both cases:\n",
        "\n",
        "* $\\large O,A$ are osbervation and action space, instead $\\large ℝ^n$ represents the continuous real space of dimension *n*, where *n* is the action dimension   \n",
        "* $\\large a_{\\text{low}}, a_{\\text{high}}$ are the action space boundaries that defines the set of admissible actions, and it's distinctive of the environment.\n",
        "* $\\Large \\mu_{\\theta}$ is the actor network \n",
        "* $\\large ϵ $ is random sample drawn from normally distributed noise with zero mean and $\\large σ$ standard deviation.\n",
        "* $\\large o_t, h^l_t$ are observation and history length at time *t*.\n",
        "\n",
        "Equations below describes the case in which policy has been used for interacting with environment, because during train also random noise will be clipped. \n",
        "\n",
        "---\n",
        "\n",
        "##### Continuous  \n",
        "\n",
        "$$ \\boxed{\\text{Equations describing how is composed continuous policy}} $$\n",
        "$$ ⇓ $$\n",
        "\n",
        "$$\\boxed{\\large  \\mu_{θ}: O, A → ℝ^n \\\\  \n",
        "\\mu_{θ}(o_t, h^l_t) = {\\textbf{a}}}$$\n",
        "\n",
        "$$ ⇓ $$\n",
        "<center><strong>Exploration</strong></center>\n",
        "$$\\boxed{\\large {\\textbf{a}} = \\text{clip}({\\textbf{a}} + ϵ, a_{\\text{low}},a_{\\text{high}}), \\;\\;\\;\\; ϵ ∼ 𝑁(0, σ)}$$\n",
        "\n",
        "*contnuous_act* method implements these operations.\n",
        "\n",
        "---\n",
        "\n",
        "##### Discrete\n",
        "\n",
        "Here two steps occurs during action generation, and the whole process defines the *Wolpertinger policy*.\n",
        "First step produce a continuous action and the second maps this action into one of the possible action that belongs to discrete set.\n",
        "Crucial here is to choose an appropriate way to represent the action space. In this project has been  proposed to use one dimensional linear space subdivided through evenly spaced points, representing the finite set of actions. So having this in mind, the proto-action has been threated as a point in this definded space, from which has been applied *k nearest neighbourhood* in order to find the *k* closest actions. *k* is expressed as a ratio having values between zero and one, that multiplied with number of actions give as result the  action subset.\n",
        "At the end, the action with highest Q-value, calculated by critic network, has been selected and then applied to environment. \n",
        "This approach allows for generalization over the action set in logarithm time as the number of actions growth, differently from general case in which the complexity grows linearly.  \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Prh5Y5MNbYHV3mv0ZrEbuQAConNWDzgO\" \n",
        "    width=\"350\" height=\"500\" align=\"left\" style=\"margin:10px;\"/>\n",
        "\n",
        "\n",
        "$$ \\boxed{\\text{Equations describing how is composed discrete policy}} $$\n",
        "$$ ⇓ $$\n",
        "\n",
        "$$\\boxed{\\large  \\mu_{θ}: O, A → ℝ^n \\\\  \n",
        "\\mu_{θ}(o_t, h^l_t) = \\hat{\\textbf{a}}, \\;\\;\\;\\; \\hat{\\textbf{a}}=\\text{proto-action}}$$\n",
        "\n",
        "$$ ⇓ $$\n",
        "<center><strong>Exploration</strong></center>\n",
        "$$\\boxed{\\large \\hat{\\textbf{a}} = \\text{clip}(\\hat{\\textbf{a}} + ϵ, a_{\\text{low}},a_{\\text{high}}), \\;\\;\\;\\; ϵ ∼ 𝑁(0, σ)}$$\n",
        "$$ ⇓ $$\n",
        "\n",
        "$$\\boxed{\\large g: ℝ^n → A \\\\\n",
        "g_k(\\hat{\\textbf{a}}) = \\text{arg}^k \\text{min}_{a∈A}|\\textbf{a}-\\hat{\\textbf{a}}|_2}$$  \n",
        "\n",
        "$$g_k ∘ \\mu_{θ} = A_k, \\; \\text{k closest action to proto-action by} L_2 \\text{distance}$$\n",
        "\n",
        "$$ ⇓ $$\n",
        "\n",
        "$$\\boxed{\\large π_{θ}(\\textbf{O}, \\textbf{H}_l) = \\text{arg max}_{a ∈ A_k} Q_{\\phi_1}(O, a, H_l)}$$  \n",
        "$$ \\large \\pi_{θ} =  \\text{Full Wolpertinger policy}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<br clear=\"left\" /> \n",
        "\n",
        "> Fig.3 Wolpertinger policy\n",
        "\n",
        "It is important to highlight that the final *Wolpertinger* policy $\\pi_{θ}$ depends only on $θ$ parameters of actor network. This beacuse in order to make this policy differentiable both $g_k$ and $\\text{argmax}$ have to be intended as deterministic effects of the environment, so could be not considered during policy training.\n",
        "\n",
        "\n",
        "*wolp_act* method include all these operations. Here the most computational part involves in replicating input data k-times, since it is necessary to evaluate Q-value for each action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZa_gj0GK-dz"
      },
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4gY0Cu39_qS8"
      },
      "outputs": [],
      "source": [
        "class ActorCriticAgent(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Combined actor-critic network.\n",
        "    According to TD3 algorithm one actor and two critic network are needed.\n",
        "    Two type of agent are possible: main and target. They differ for how an action has been selected.\n",
        "    For main agent the output of actor network has been added to an exploration noise with defined standard deviation.\n",
        "    Instead, for target type the noise, with its defined standard deviation, has to be clipped before to be added\n",
        "    to actor output network, according to target policy smoothing described in TD3 algorithm.\n",
        "    Furthermore, TD3 relies on DDPG that works only for continuous action space, so in order to deal also with discrete\n",
        "    action space, this implementation of TD3 has been extended including also Wolpertinger policy.\n",
        "    This variation, after the applications of noise to actor output(the so-called proto-action), performs a further step\n",
        "    embedding the proto-action into a k-nearest-neighbor mapping, that reduce the continuous proto-action into a\n",
        "    discrete set.\n",
        "    In both continuous and discrete action space this action selection has been performed by main and target agent,\n",
        "    respectively during environment interaction and during the update of critic parameters. Instead, in both cases\n",
        "    the parameters of actor network are updated taking into account the output of actor network without adding any kind\n",
        "    of noise.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim, config_net, act_dim, act_space_cont, act_up_limit, act_low_limit,\n",
        "                 actions, knn_ratio, st_dev_noise, clip_noise, hist_len, agent_type):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.agent_type = agent_type\n",
        "        self.actor = ActorCriticModel(obs_dim, config_net[\"actor_net\"], act_dim, act_space_cont, model_type=\"actor\")\n",
        "        self.critic_1 = ActorCriticModel(obs_dim, config_net[\"critic_net\"], act_dim, act_space_cont,\n",
        "                                         model_type=\"critic\")\n",
        "        self.critic_2 = ActorCriticModel(obs_dim, config_net[\"critic_net\"], act_dim, act_space_cont,\n",
        "                                         model_type=\"critic\")\n",
        "\n",
        "        self.upper_act_limit = act_up_limit\n",
        "        self.lower_act_limit = act_low_limit\n",
        "        self.actions_space_continuous = act_space_cont\n",
        "        self.std_deviation_act_noise = st_dev_noise\n",
        "        self.clip_noise = clip_noise\n",
        "        self.hist_length = hist_len\n",
        "        # self.random_process = ornstein_uhlenbeck_process(initial_value=0.0, stddev=st_dev_noise)\n",
        "\n",
        "        if not self.actions_space_continuous:\n",
        "            num_actions = actions\n",
        "            self.action_space = Discrete_space(num_actions)\n",
        "            self.knn = max(1, int(num_actions * knn_ratio))\n",
        "            self.knn_tensor = tf.constant(self.knn, dtype=tf.int32)\n",
        "\n",
        "    @tf.function\n",
        "    def get_action(self, input_act_net):\n",
        "        if self.actions_space_continuous:\n",
        "            act = self.continuous_act(input_act_net)\n",
        "            raw_act = act\n",
        "            act = tf.squeeze(act)\n",
        "        else:\n",
        "            act, raw_act = self.wolp_act(input_act_net)\n",
        "            act = tf.squeeze(act)\n",
        "        return act, raw_act\n",
        "\n",
        "    def continuous_act(self, inputs_act_net):\n",
        "        action = self.actor(inputs_act_net)\n",
        "\n",
        "        if self.agent_type == \"main\":\n",
        "            action = tf.math.add(action, tf.random.normal(shape=action.shape, stddev=self.std_deviation_act_noise))\n",
        "\n",
        "            action = tf.clip_by_value(action, clip_value_min=self.lower_act_limit, clip_value_max=self.upper_act_limit)\n",
        "\n",
        "        elif self.agent_type == \"target\":\n",
        "            target_policy_smooth = tf.random.normal(shape=action.shape, stddev=self.std_deviation_act_noise)\n",
        "            target_policy_smooth = tf.clip_by_value(target_policy_smooth, clip_value_min=-self.clip_noise,\n",
        "                                                    clip_value_max=self.clip_noise)\n",
        "            action = tf.math.add(action, target_policy_smooth)\n",
        "            action = tf.clip_by_value(action, clip_value_min=self.lower_act_limit, clip_value_max=self.upper_act_limit)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def wolp_act(self, inputs_act_net):\n",
        "\n",
        "        proto_action = self.actor(inputs_act_net)\n",
        "\n",
        "        if self.agent_type == \"main\":\n",
        "            proto_action = tf.math.add(proto_action, tf.random.normal(shape=proto_action.shape,\n",
        "                                                                       stddev=self.std_deviation_act_noise))\n",
        "            #proto_action = tf.math.add(proto_action, self.random_process())\n",
        "            proto_action = tf.clip_by_value(proto_action, clip_value_min=self.lower_act_limit,\n",
        "                                            clip_value_max=self.upper_act_limit)\n",
        "            # action = tf.math.add(action, self.random_process())\n",
        "        elif self.agent_type == \"target\":\n",
        "            target_policy_smooth = tf.random.normal(shape=proto_action.shape, stddev=self.std_deviation_act_noise)\n",
        "            # target_policy_smooth = self.random_process()\n",
        "            target_policy_smooth = tf.clip_by_value(target_policy_smooth, clip_value_min=-self.clip_noise,\n",
        "                                                    clip_value_max=self.clip_noise)\n",
        "            proto_action = tf.math.add(proto_action, target_policy_smooth)\n",
        "            proto_action = tf.clip_by_value(proto_action, clip_value_min=self.lower_act_limit,\n",
        "                                            clip_value_max=self.upper_act_limit)\n",
        "\n",
        "        proto_action_input = tf.cast(proto_action, dtype=tf.float64)\n",
        "        # Raw actions are real values that represent one of the possible discrete set of actions.\n",
        "        # Actions are the integer values that represent one of the possible actions.\n",
        "        # Raw actions are used to be stored in replay buffer and used to feed networks, instead actions are used\n",
        "        #  to interact with environment.\n",
        "        raw_actions, actions = self.action_space.tf_search_point(proto_action_input, self.knn_tensor)\n",
        "\n",
        "        obs = inputs_act_net[\"features\"][\"obs\"]\n",
        "        last_obs = inputs_act_net[\"memory\"][\"obs\"]\n",
        "        last_act = inputs_act_net[\"memory\"][\"act\"]\n",
        "        num_raw_actions = tf.cast(obs.get_shape()[0], dtype=tf.int64)\n",
        "\n",
        "        raw_actions = tf.cast(raw_actions, dtype=tf.float32)\n",
        "\n",
        "        if self.knn > 1:\n",
        "            # Since if knn is greater than one for each observation, along with its history sequence, there will be a\n",
        "            # raw actions equal to knn. So, in order to evaluate which raw action is the best, i.e. gave highest\n",
        "            # Q value, it's necessary to tile, according to the value of knn, each observation and its history sequence,\n",
        "            # in order to correctly form pairs of observation and raw action.\n",
        "            # The principle of tile process is the same both in inference that update case, but this last one\n",
        "            # is more complicated due to ragged tensor, so further operations have to be done.\n",
        "\n",
        "            obs_dim, act_dim = last_obs.shape[2], last_act.shape[2]\n",
        "            s_t = tf.tile(obs, tf.constant(value=[1, self.knn]))\n",
        "            s_t = tf.reshape(s_t, shape=[num_raw_actions, self.knn, obs_dim])\n",
        "\n",
        "            if isinstance(last_obs, tf.RaggedTensor) and isinstance(last_act, tf.RaggedTensor):  # Update case\n",
        "                last_obs_not_ragged, last_act_not_ragged = last_obs.to_tensor(), last_act.to_tensor()\n",
        "                max_hist_len = tf.cast(self.hist_length, dtype=tf.int64)\n",
        "\n",
        "                # Create boolean mask\n",
        "                # Making ones\n",
        "                total_ones = tf.reduce_sum(last_obs.row_lengths())\n",
        "                ragged_ones = tf.RaggedTensor.from_row_lengths(tf.ones([total_ones]),\n",
        "                                                               row_lengths=last_obs.row_lengths())\n",
        "                # Making zeros\n",
        "                total_zeros = (max_hist_len * num_raw_actions) - total_ones\n",
        "                zeros_row_lengths = max_hist_len - last_obs.row_lengths()\n",
        "                ragged_zeros = tf.RaggedTensor.from_row_lengths(tf.zeros([total_zeros]), row_lengths=zeros_row_lengths)\n",
        "\n",
        "                bool_mask = tf.cast(tf.concat([ragged_ones, ragged_zeros], axis=1), dtype=tf.bool)\n",
        "                bool_mask = bool_mask.to_tensor()\n",
        "                bool_mask = tf.tile(bool_mask, tf.constant(value=[1, self.knn]))\n",
        "                bool_mask = tf.reshape(bool_mask, shape=[num_raw_actions, self.knn, max_hist_len])\n",
        "\n",
        "                # Apply boolean mask to last_obs  and last_act tiled and reshaped\n",
        "                last_obs_not_ragged_tile = tf.tile(last_obs_not_ragged, tf.constant(value=[1, self.knn, 1]))\n",
        "                last_obs_not_ragged_reshaped = tf.reshape(last_obs_not_ragged_tile, shape=[num_raw_actions, self.knn,\n",
        "                                                                                           max_hist_len,\n",
        "                                                                                           obs_dim])\n",
        "\n",
        "                last_act_not_ragged_tile = tf.tile(last_act_not_ragged, tf.constant(value=[1, self.knn, 1]))\n",
        "                last_act_not_ragged_reshaped = tf.reshape(last_act_not_ragged_tile, shape=[num_raw_actions, self.knn,\n",
        "                                                                                           max_hist_len,\n",
        "                                                                                           act_dim])\n",
        "                # Obtain reshaped and tiled last_obs and last_act\n",
        "                last_obs = tf.ragged.boolean_mask(last_obs_not_ragged_reshaped, bool_mask)\n",
        "                last_act = tf.ragged.boolean_mask(last_act_not_ragged_reshaped, bool_mask)\n",
        "\n",
        "            elif isinstance(last_obs, tf.Tensor) and isinstance(last_act, tf.Tensor):  # Inference case\n",
        "                last_obs_tiled = tf.tile(last_obs, tf.constant(value=[1, self.knn, 1]))\n",
        "                last_act_tiled = tf.tile(last_act, tf.constant(value=[1, self.knn, 1]))\n",
        "                last_obs = tf.reshape(last_obs_tiled, shape=[num_raw_actions, self.knn,\n",
        "                                                             last_obs.shape[1], obs_dim])\n",
        "                last_act = tf.reshape(last_act_tiled, shape=[num_raw_actions, self.knn,\n",
        "                                                             last_act.shape[1], act_dim])\n",
        "\n",
        "            fn_to_map = lambda i: self.critic_1(dict(memory={\"obs\": last_obs[i],\n",
        "                                                             \"act\": last_act[i]}, features={\"obs\": s_t[i],\n",
        "                                                                                            \"act\": raw_actions[i]}))\n",
        "            actions_evaluation = tf.map_fn(fn=fn_to_map, elems=tf.range(num_raw_actions), dtype=tf.float32)\n",
        "\n",
        "            # Return the best action, i.e., wolpertinger action from the full wolpertinger policy\n",
        "            max_index = tf.math.argmax(actions_evaluation, axis=1)\n",
        "            raw_actions_max = tf.squeeze(tf.gather(raw_actions, indices=max_index, batch_dims=1), axis=1)\n",
        "            actions_max = tf.gather(actions, indices=max_index, batch_dims=1)\n",
        "\n",
        "        else:\n",
        "            raw_actions_max = raw_actions\n",
        "            actions_max = actions\n",
        "\n",
        "        return actions_max, raw_actions_max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFKSjBOpoH6N"
      },
      "source": [
        "### Replay buffer \n",
        "\n",
        "\n",
        "---\n",
        "In general reinforcement learning algorithm are divided in two logical phases, one in which the agent interact with environment collecting experience, and one in which the agent train network parameters using this data. Data could be stored or not depending on algorithm design, for the proposed algorithm data will be stored in a so called Replay Buffer memory, where replay indicates that same experience could be used multiple times during training.    \n",
        "Replay buffer is a typical tool that is used in alghoritm that learn to approximate optimal action-value function **Q<sup>*</sup>**, like in this case.\n",
        "This memory stores all agent's experience in form of tuple: *(o<sub>t</sub>,  a<sub>t</sub>, r<sub>t</sub>, o<sub>t+1</sub>, d<sub>t</sub>)*, where *d<sub>t</sub>* indicates if the terminal state is reached after observing *o<sub>t+1</sub>*, terminal state indicating that current episode is finished.  \n",
        "It has been introduced in order to improve stability during training process, and usually it should be large enough to store a wide range of experiences.\n",
        "Since the same replay buffer is used among different episodes, could happens that, during the sample of past history, experience from different episodes could be collected as well. So, in order to avoid this phenomen batches of past history have been designed using ragged tensor provided by Tensorflow, forming batched tensor with variable sequences length, that properly reflects the past history of observations/actions sampled at time *t*.\n",
        "     \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW-MU6p24umC"
      },
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bDSbgMk94pqb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, obs_dim, act_dim, capacity, device):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.batch_size = 1  # Because one element is added for each episode step\n",
        "        self.batch_length = capacity\n",
        "        data_spec = (\n",
        "            tf.TensorSpec([obs_dim], dtype=tf.float32, name='observation'),\n",
        "            tf.TensorSpec([act_dim], dtype=tf.float32, name='action'),\n",
        "            tf.TensorSpec([], dtype=tf.float32, name='reward'),\n",
        "            tf.TensorSpec([obs_dim], dtype=tf.float32, name='next_observation'),\n",
        "            tf.TensorSpec([], dtype=tf.float32, name='done'),\n",
        "        )\n",
        "        self.buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "            data_spec, batch_size=self.batch_size, max_length=self.batch_length, device=device)\n",
        "\n",
        "    def put(self, observation, action, reward, next_observation, done):\n",
        "        self.buffer.add_batch((observation, action, reward, next_observation, done))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_boolean_mask(done_sequence, batch_size, max_hist_len):\n",
        "        \"\"\"\n",
        "        This method create a ragged tensor of indices starting from the done sequence preceding elements at\n",
        "        given time t. This serves to correctly associate, for each element, its experience, in order to\n",
        "        not include also elements belonging to other episodes.\n",
        "        For example if done batch sequence is [[0, 1, 0, 0, 0], [0, 0, 0, 1, 0]] the returned ragged tensor should be\n",
        "        [[2, 3, 4], [4]], since elements where done is 1 are not considered.\n",
        "        The special case in which one sequence could end with 1, means that element at time t represent the first one\n",
        "        in the sampled episode, so no prior experience could be retrieved. In this case also the indices at which this\n",
        "        happens will be returned in order to add a zero sequence of length one in the final batched sample.\n",
        "\n",
        "        :param done_sequence: tensor of dimension [batch_size, max_hist_len]\n",
        "        :param batch_size: number of rows for done batch\n",
        "        :param max_hist_len: number of columns for done batch\n",
        "        :return: ragged tensor of indices of dimension [batch_size, None], rows indices where no prior experience occurs\n",
        "        \"\"\"\n",
        "        coordinates_true_done = tf.where(done_sequence == 1)\n",
        "        unique_obj = tf.unique_with_counts(coordinates_true_done[:, 0])\n",
        "        rows_with_ones = unique_obj.y\n",
        "\n",
        "        lengths_rows_with_ones = unique_obj.count\n",
        "        row_lengths = tf.zeros(shape=[batch_size], dtype=tf.int32)\n",
        "        row_lengths = tf.tensor_scatter_nd_update(row_lengths, tf.expand_dims(rows_with_ones, axis=-1),\n",
        "                                                  lengths_rows_with_ones)\n",
        "\n",
        "        tmp = tf.RaggedTensor.from_row_lengths(coordinates_true_done, row_lengths)\n",
        "        max_index_for_row = tf.reduce_max(tmp, axis=1)[:, 1:]\n",
        "        max_index_for_row = tf.where(max_index_for_row >= 0, tf.math.add(max_index_for_row, 1), 0)\n",
        "\n",
        "        row_indexes_no_mem = tf.where(max_index_for_row[:, 0] == max_hist_len)\n",
        "        tmp_mod = tf.where(max_index_for_row == max_hist_len, tf.math.add(max_index_for_row, -1), max_index_for_row)\n",
        "        ragged_indices_to_preserve = tf.ragged.range(starts=tmp_mod[:, 0], limits=max_hist_len)\n",
        "\n",
        "        return ragged_indices_to_preserve, row_indexes_no_mem\n",
        "\n",
        "    @tf.function\n",
        "    def sample_batch_with_history(self, batch_size, max_hist_len):\n",
        "        \"\"\"\n",
        "        Get a random batch sample. The total sequence length sampled is  max_hist_len + 2 , this because\n",
        "        max_hist_len elements represent the history of observation at time t, present at max_hist_len + 1.\n",
        "        The last max_hist_len + 2 index represent elements at time t+1, used for fetch history of action at time t+1.\n",
        "        Gave as result a dictionary including ragged batched tensor for each element stored in replay buffer.\n",
        "\n",
        "        :param batch_size: number of element in batch\n",
        "        :param max_hist_len: sequence length\n",
        "        :return: dictionary including the sampled quantities\n",
        "        \"\"\"\n",
        "\n",
        "        sampled_batch_with_history = self.buffer.get_next(sample_batch_size=batch_size, num_steps=max_hist_len + 2)\n",
        "\n",
        "        # According to data spec of replay buffer in constructor\n",
        "        # 0 - Observation at time t\n",
        "        # 1 - Action at time t\n",
        "        # 2 - Reward at time t\n",
        "        # 3 - Observation at time t+1\n",
        "        # 4 - Done signal at time t\n",
        "\n",
        "        obs_batch = sampled_batch_with_history[0][0][:, -2, :]\n",
        "        act_batch = sampled_batch_with_history[0][1][:, -2, :]\n",
        "        rew_batch = tf.expand_dims(sampled_batch_with_history[0][2][:, -2], axis=-1)\n",
        "        next_obs_batch = sampled_batch_with_history[0][3][:, -2, :]\n",
        "        done_batch = tf.expand_dims(sampled_batch_with_history[0][4][:, -2], axis=-1)\n",
        "\n",
        "        # Two boolean mask are needed, one for time t and one for time t+1\n",
        "        done_history_seq_t = sampled_batch_with_history[0][4][:, :max_hist_len]\n",
        "        done_history_seq_next_t = sampled_batch_with_history[0][4][:, 1:max_hist_len + 1]\n",
        "\n",
        "        range_indices_t, indexes_no_mem_t = self.get_boolean_mask(done_history_seq_t, batch_size, max_hist_len)\n",
        "        range_indices_next_t, indexes_no_mem_next_t = self.get_boolean_mask(done_history_seq_next_t, batch_size,\n",
        "                                                                            max_hist_len)\n",
        "\n",
        "        hist_obs_batch = sampled_batch_with_history[0][0][:, :max_hist_len, :]\n",
        "        hist_act_batch = sampled_batch_with_history[0][1][:, :max_hist_len, :]\n",
        "        hist_next_obs_batch = sampled_batch_with_history[0][3][:, :max_hist_len, :]\n",
        "        hist_next_act_batch = sampled_batch_with_history[0][1][:, 1:max_hist_len + 1, :]\n",
        "\n",
        "        if len(indexes_no_mem_t) != 0:\n",
        "            num_seq_to_update = len(indexes_no_mem_t)\n",
        "            hist_obs_batch = tf.tensor_scatter_nd_update(hist_obs_batch,\n",
        "                                                         indexes_no_mem_t,\n",
        "                                                         tf.zeros([num_seq_to_update, max_hist_len, self.obs_dim]))\n",
        "            hist_act_batch = tf.tensor_scatter_nd_update(hist_act_batch,\n",
        "                                                         indexes_no_mem_t,\n",
        "                                                         tf.zeros([num_seq_to_update, max_hist_len, self.act_dim]))\n",
        "\n",
        "        if len(indexes_no_mem_next_t) != 0:\n",
        "            num_seq_to_update = len(indexes_no_mem_next_t)\n",
        "            hist_next_obs_batch = tf.tensor_scatter_nd_update(hist_next_obs_batch,\n",
        "                                                              indexes_no_mem_next_t,\n",
        "                                                              tf.zeros([num_seq_to_update, max_hist_len, self.obs_dim]))\n",
        "\n",
        "            hist_next_act_batch = tf.tensor_scatter_nd_update(hist_next_act_batch,\n",
        "                                                              indexes_no_mem_next_t,\n",
        "                                                              tf.zeros([num_seq_to_update, max_hist_len, self.act_dim]))\n",
        "\n",
        "        hist_obs_batch = tf.gather(hist_obs_batch, range_indices_t, batch_dims=1)\n",
        "        hist_act_batch = tf.gather(hist_act_batch, range_indices_t, batch_dims=1)\n",
        "        hist_next_obs_batch = tf.gather(hist_next_obs_batch, range_indices_next_t, batch_dims=1)\n",
        "        hist_next_act_batch = tf.gather(hist_next_act_batch, range_indices_next_t, batch_dims=1)\n",
        "\n",
        "        batch_sampled = {\"obs\": obs_batch,\n",
        "                         \"act\": act_batch,\n",
        "                         \"rew\": rew_batch,\n",
        "                         \"next_obs\": next_obs_batch,\n",
        "                         \"done\": done_batch,\n",
        "                         \"hist_obs\": hist_obs_batch,\n",
        "                         \"hist_act\": hist_act_batch,\n",
        "                         \"hist_next_obs\": hist_next_obs_batch,\n",
        "                         \"hist_next_act\": hist_next_act_batch}\n",
        "\n",
        "        return batch_sampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJCZRe763dro"
      },
      "source": [
        "### Training \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXLPqC8m29l"
      },
      "source": [
        "#### Critic \n",
        "\n",
        "---\n",
        "\n",
        "TD3 concurrently learns two **Q**-functions, $Q_{\\phi_1}$ and $Q_{\\phi_2}$, by mean square Bellman error minimization on a randomly sampled batch from replay buffer. Since TD3 provides two critic, so two distinct loss functions are needed, but only one target $y$ is used and that is the same for both.\n",
        "\n",
        "$\\Large L_{\\text{critic}}(\\phi_j, {\\mathcal B})_{j=1,2} = E_{ \\{(h^l_t,h^l_{t+1},o_t,a_t,r_t,o_{t+1},d_t)_i\\}^{|{\\mathcal B}|}_{i=1}}\n",
        "{\\Big( Q_{\\phi_j}(o_t,a_t,h^l_t) - y(r_t,d_t,o_{t+1},h^l_{t+1}) \\Bigg)^2}  \n",
        "$\n",
        "> Eq.1 Loss Function\n",
        "\n",
        "\n",
        "* ${\\mathcal B}$  is the sampled batch from replay buffer. |${\\mathcal B}$| is the cardinality.\n",
        "* $Q_{\\phi_j}$ is the prediction of the Q-value function done by main critic network evaluated at time *t*  \n",
        "* $y$ is the Bellman lookup target  \n",
        "\n",
        "\n",
        "$\\Large y(r_t,d_t,o_{t+1},h^l_{t+1}) =  r_t + γ*(1-d_t)* \\text{min}_{j=1,2}Q_{\\phi_{j,\\text{targ}}}(o_{t+1},a_{t+1},h^l_{t+1})$\n",
        "\n",
        "> Eq.2 Target \n",
        "\n",
        "* $r_t$ is the reward at time *t*\n",
        "* $ γ$ is the discount factor, indicating how much important are the future returns. Its value is included between 0 and 1.\n",
        "* $ d_t$ boolean value, indicatin if the *t+1* observation led to terminal state. If 1 future rewards are discarded.\n",
        "* $Q_{\\phi_{j,targ}}$ is the Q-value prediction done by critic target networks.\n",
        "Target network have been introduced in order to stabilize learning process, further TD3 doubled the critic target networks and taking the smaller between them, helps fend off overestimation in the Q-function.\n",
        "*$a_{t+1}$ is action at time *t+1* based on target policy $μ_{θ_{targ}}$. Instead for **discret** action space it is based on the full wolpertinger policy $\\pi_{θ_{targ}}$ \n",
        "\n",
        "$\\Large a_{t+1} = \\text{clip}\\left(\\mu_{\\theta_{\\text{targ}}}(o_{t+1},h^l_{t+1}) + \\text{clip}(ϵ,-c,c), a_{Low}, a_{High}\\right), \\;\\;\\;\\;\\; \\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{targ}})$\n",
        "\n",
        "> Eq 3. Target policy smoothing\n",
        "\n",
        "* $c$ clipping value for normally distributed noise. This ensure to not obtain action that are too far from the one predicted by target policy.\n",
        "* $a_{Low}, a_{High}$ are boundaries that define the set of valid actions for the current environment.  \n",
        "\n",
        "<br>\n",
        "\n",
        "$\\Large \\text{min}_{ϕ_j} \\; \\{L_{\\text{critic}}(\\phi_j, {\\mathcal B}) \\}_{j=1,2}$\n",
        "> Eq. 4 Optimization\n",
        "\n",
        "In the end ${ϕ_1}$ and ${ϕ_2}$ parameters are optimized to minimize the related loss function. In this case gradient descent has been performed, calculating the gradient taking into account only parameters of the main networks, ${ϕ_1}$ and ${ϕ_2}$, instead ${ϕ_{1,\\text{targ}}}$,${ϕ_{2,\\text{targ}}}$ and ${θ_{\\text{targ}}}$ are treated as constant useful for constructing the target. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io8109xBQnkY"
      },
      "source": [
        "##### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eZ2gACmeQqrL"
      },
      "outputs": [],
      "source": [
        "def update_critic_parameters(data, optimizer, discount_factor, main_agent, target_agent):\n",
        "    # Input at time t\n",
        "    inputs_obs = {\"memory\": {\"obs\": data['hist_obs'],\n",
        "                             \"act\": data['hist_act']},\n",
        "                  \"features\": {\"obs\": data['obs'],\n",
        "                               \"act\": data['act']}}\n",
        "\n",
        "    # Input at time t+1\n",
        "    inputs_next_obs = {\"memory\": {\"obs\": data['hist_next_obs'],\n",
        "                                  \"act\": data['hist_next_act']},\n",
        "                       \"features\": {\"obs\": data['next_obs'],\n",
        "                                    \"act\": None}}\n",
        "    rewards_batch = data[\"rew\"]\n",
        "    done_batch = data[\"done\"]\n",
        "\n",
        "    _, next_act = target_agent.get_action(inputs_next_obs)\n",
        "\n",
        "    inputs_next_obs[\"features\"][\"act\"] = next_act\n",
        "    # Target Q-values\n",
        "    q1_pi_targ = target_agent.critic_1(inputs_next_obs)\n",
        "    q2_pi_targ = target_agent.critic_2(inputs_next_obs)\n",
        "    q_pi_targ = tf.math.minimum(q1_pi_targ, q2_pi_targ)\n",
        "\n",
        "    backup = rewards_batch + discount_factor * (1 - done_batch) * q_pi_targ\n",
        "\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    # Compute critic loss for both critic network for main agent\n",
        "    with tf.GradientTape() as tape:\n",
        "        q1 = main_agent.critic_1(inputs_obs)\n",
        "        q2 = main_agent.critic_2(inputs_obs)\n",
        "\n",
        "        # MSE loss against Bellman backup\n",
        "        loss_q1 = mse(backup, q1)\n",
        "        loss_q2 = mse(backup, q2)\n",
        "        loss_q = loss_q1 + loss_q2\n",
        "\n",
        "    # Update trainable parameters\n",
        "    q1_q2_parameters = main_agent.critic_1.trainable_variables + main_agent.critic_2.trainable_variables\n",
        "    grads = tape.gradient(loss_q, q1_q2_parameters)\n",
        "    optimizer.apply_gradients(zip(grads, q1_q2_parameters))\n",
        "\n",
        "    return loss_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-KQ5HzRQAY"
      },
      "source": [
        "#### Actor\n",
        "\n",
        "---\n",
        "\n",
        "The policy network $μ_{θ}$  update aims to optimize parameters in order to find actions that maximize $Q_{\\phi}$. Since the action space is continuous, it is possible to assume that $Q_{\\phi}$ is differentiable respect to action, so performing a gradient ascent respect to the expression below will solve the optimization problem. This remains valid also for discrete action space, since for training is considered only $μ_{θ}$, that produces a continuous value in $ℝ^n$ dimension, and not the full Wolpertinger policy $\\pi_{θ}$.\n",
        "\n",
        "$\\Large \n",
        "L_{\\text{actor}}(θ,{\\mathcal B})=  E_{ \\{(h^l_t,o_t)_i\\}^{|{\\mathcal B}|}_{i=1}}Q_{ϕ_1}(h^l_t,μ_{θ}(h^l_t,o_t),o_t), \\;\\;\\;\\;\\; \\text{max}_{\\theta} L_{\\text{actor}}(θ,{\\mathcal B})$\n",
        "\n",
        "> Eq 5. Optimization for Actor loss function\n",
        "\n",
        "The gradient is calculated only respect to $θ$ parameters of policy network, where $\\phi_1$ parameters are treated as constants. Nevertheless TD3 algorithm provides two critic networks, the first paper suggests to take into account $\\phi_1$ parameters respect to $\\phi_2$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYWZ7jdj4buK"
      },
      "source": [
        "##### Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h-1w4AVN4eBr"
      },
      "outputs": [],
      "source": [
        "def update_actor_parameters(data, optimizer, main_agent):\n",
        "    inputs_obs = {\"memory\": {\"obs\": data['hist_obs'],\n",
        "                             \"act\": data['hist_act']},\n",
        "                  \"features\": {\"obs\": data['obs'],\n",
        "                               \"act\": None}}\n",
        "\n",
        "    # Compute actor loss for actor network. Here only parameters of actor net are watched by tape since critic\n",
        "    # parameters are not required to be updated, but only used for compute the actor loss.\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "        tape.watch(main_agent.actor.trainable_variables)\n",
        "        act_logits = main_agent.actor(inputs_obs)\n",
        "        inputs_obs[\"features\"][\"act\"] = act_logits\n",
        "        q1_pi = main_agent.critic_1(inputs_obs)\n",
        "        loss_act = tf.reduce_mean(-q1_pi)\n",
        "\n",
        "    grads = tape.gradient(loss_act, main_agent.actor.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, main_agent.actor.trainable_variables))\n",
        "\n",
        "    return loss_act"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoJlh_itoXbH"
      },
      "source": [
        "#### Target \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "As TD3 algorithm describes the set of target parameters are updated less frequently respect to main parameters, following this expressions:  \n",
        "\n",
        "$ \\Large \\{ ϕ_{\\text{j,targ}} ← τ* ϕ_j + (1 - τ)*ϕ_{\\text{j,targ}} \\}_{j=1,2}$ \n",
        "\n",
        "$ \\Large \\theta_{\\text{targ}} ← τ* \\theta + (1 - τ)*\\theta_{\\text{targ}}$ \n",
        "\n",
        "> Eq.6 Soft Update target parameters\n",
        "\n",
        "τ is a small number that performs a movin average on previous target parameters.\n",
        "This is usually called *soft update*, that differs from *hard update* in which target parameters are updated in order to exactly  match values of main parameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9NoHgi4sOVd"
      },
      "source": [
        "#### Update\n",
        "\n",
        "The method shows the update process in its entirety, including method for update critic networks, along with delayed update of target and actor networks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6a8uL2rwsQzH"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def update(main_agent, target_agent, upd_counter, batch_sizes, max_history_length,\n",
        "           freq_policy_update, tau_target_update, disc_factor, critic_optimizer, actor_optimizer):\n",
        "    loss_q, loss_a = tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
        "    sampled_batch = replay_buffer.sample_batch_with_history(batch_size=batch_sizes,\n",
        "                                                            max_hist_len=max_history_length)\n",
        "\n",
        "    loss_q = update_critic_parameters(sampled_batch, critic_optimizer, disc_factor, main_agent, target_agent)\n",
        "\n",
        "    # Delayed policy updates\n",
        "    if upd_counter % freq_policy_update == 0:\n",
        "        loss_a = update_actor_parameters(sampled_batch, actor_optimizer, main_agent)\n",
        "\n",
        "        # Soft update of target parameters according to small tau value\n",
        "        for par_main, par_targ in zip(main_agent.trainable_variables,\n",
        "                                      target_agent.trainable_variables):\n",
        "            par_targ.assign(tau_target_update * par_main + (1 - tau_target_update) * par_targ)\n",
        "\n",
        "    return loss_q, loss_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDJCm6F63ZGc"
      },
      "source": [
        "### Algorithm \n",
        "\n",
        "The next sections will describe the TD3 algorithm in its entirety, including replay buffer initialization, environment interaction and the already explained update phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaBUKAx25f7c"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1UcDuD5zEvF_JxspXMM_LuHtWdrLYuMoY\"    width=\"700\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v4b9kN7EqUp"
      },
      "source": [
        "#### Replay buffer initialization\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "As described in pseudo algorithm replay buffer is filled with randomly collected experience in order to provide some initial memory for future training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XFypu80VE_Lo"
      },
      "outputs": [],
      "source": [
        "def run_warmup_episodes(num_episodes_warmup):\n",
        "    \"\"\"\n",
        "    Warmup episodes are executed in order to feed Replay buffer memory.\n",
        "    Randomly selected actions used for interact with environment.\n",
        "    :param num_episodes_warmup:\n",
        "    \"\"\"\n",
        "    with tqdm.trange(num_episodes_warmup) as warm_episodes:\n",
        "        for warm_ep in warm_episodes:\n",
        "            obs_warm = tf.constant(env.reset(), shape=(1, obs_dim), dtype=tf.float32)\n",
        "            for warm_step in range(episode_steps):\n",
        "                if continuous:\n",
        "                    action = tf.convert_to_tensor(env.action_space.sample())\n",
        "                    raw_action = tf.expand_dims(action, axis=0)\n",
        "                else:\n",
        "                    proto_action = tf.random.uniform(minval=lower_act_limit, maxval=upper_act_limit,\n",
        "                                                     shape=(1, act_dim), dtype=tf.float64)\n",
        "                    knn = tf.constant(1, dtype=tf.int32)\n",
        "                    raw_action, action = actor_critic_main.action_space.tf_search_point(proto_action, knn)\n",
        "                    raw_action = tf.cast(raw_action, dtype=tf.float32)\n",
        "                    action = tf.squeeze(action)\n",
        "\n",
        "                next_obs_warm, reward_warm, done_warm = env.tf_step(action)\n",
        "                done_warm = tf.ones_like(done_warm) if warm_step == episode_steps - 1 else done_warm\n",
        "                replay_buffer.put(obs_warm, raw_action, reward_warm, next_obs_warm, done_warm)\n",
        "                obs_warm = next_obs_warm\n",
        "\n",
        "                if tf.cast(done_warm, dtype=tf.bool):\n",
        "                    break\n",
        "\n",
        "            warm_episodes.set_description(f\"Warmup episode:[{warm_ep}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jezEaMz1FFzw"
      },
      "source": [
        "#### Environment interaction \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This step of algorithm has been implemented in a separated function in order to optimize execution time exploiting Tensorflow graph execution.  \n",
        "\n",
        "*action* is the action format that is accepted by environment, instead *raw_action* is the action as it is returned by the policy.\n",
        "\n",
        "In this method also the update of history sequence has been done since it's relies on Tensorflow *concat* operation, that could be optimized if included in a Tensorflow function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jjMhdo4XFIg0"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def env_interaction(input_act, main_agent, buffer_l):\n",
        "    \"\"\"\n",
        "    Interaction with environment has been performed through an input dictionary that contains tensors for memory\n",
        "    and actor network. This method after the interaction returns reward step, done signal and next input\n",
        "    dictionary, in which  last buffer_l observations and actions are stored, along with observation for the next step.\n",
        "    :param input_act: input dictionary with observation at time t and previous observations and actions\n",
        "    :param main_agent: Agent that perform action\n",
        "    :param buffer_l: number of previous elements to take in consideration\n",
        "    :return: next_input dictionary for next step\n",
        "    \"\"\"\n",
        "    observation = input_act[\"features\"][\"obs\"]\n",
        "    next_input_act = {\"memory\": {\"obs\": None,\n",
        "                                 \"act\": None},\n",
        "                      \"features\": {\"obs\": None,\n",
        "                                   \"act\": None}}\n",
        "\n",
        "    action, raw_action = main_agent.get_action(input_act)\n",
        "\n",
        "    next_observation, reward, done = env.tf_step(action)\n",
        "\n",
        "    next_input_act[\"features\"][\"obs\"] = next_observation\n",
        "    last_hist_obs, last_hist_act = input_act[\"memory\"][\"obs\"], input_act[\"memory\"][\"act\"]\n",
        "\n",
        "    observation = tf.expand_dims(observation, axis=0)\n",
        "    raw_action = tf.expand_dims(raw_action, axis=0)\n",
        "    # For each iteration stack the last action and observation, keeping only the last buffer_l elements\n",
        "    next_input_act[\"memory\"][\"obs\"] = tf.concat([last_hist_obs, observation], axis=1)[:, -buffer_l:, :]\n",
        "    next_input_act[\"memory\"][\"act\"] = tf.concat([last_hist_act, raw_action], axis=1)[:, -buffer_l:, :]\n",
        "\n",
        "    return next_input_act, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myn3O49Kpy4o"
      },
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are initialized variables that are no strictly related with agents, these are more related with the expriment in general."
      ],
      "metadata": {
        "id": "gZZlmA_GROCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PBFdWYF4ibFK"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--seed', type=int, default=42, help=\"Seed for experiment reproducibility\")\n",
        "parser.add_argument('--env_name', type=str, default='HalfCheetahBulletEnv-v0', help=\"Environment name\")\n",
        "parser.add_argument('--pomdp_type', type=str, default='remove_velocity', help=\"Type of pomdp observation\")\n",
        "parser.add_argument('--config_filename', type=str, default='config_net.toml', help=\"Name configuration file\")\n",
        "#parser.add_argument('--num_episodes', type=int, default=1000, help=\"Number of episodes\")\n",
        "parser.add_argument('--num_warmup_episodes', type=int, default=10, help=\"Number of warmup episodes\")\n",
        "args = parser.parse_args(args=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below represents the environment and agents initialization, restoring previous values if related checkpoint is present."
      ],
      "metadata": {
        "id": "-Gv1ObdIP-rv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jpy8Rkq85GE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Avoid warnings to be displayed on console\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "# Create the environment\n",
        "env = POMDPWrapper(env_name=args.env_name, pomdp_type=args.pomdp_type)\n",
        "\n",
        "# Set seed\n",
        "env.seed(args.seed)\n",
        "tf.random.set_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "continuous = None\n",
        "try:  # TD3 continuous action space - normal way\n",
        "    obs_dim = env.observation_space.shape[0] if len(\n",
        "        env.observation_space.shape) != 0 else 1\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    # Not used np.inf is only to indicate that are infinite number of actions\n",
        "    num_actions = np.inf\n",
        "    upper_act_limit = env.action_space.high\n",
        "    lower_act_limit = env.action_space.low\n",
        "    continuous = True\n",
        "except IndexError:  # TD3 discrete action space using Wolpertinger agent\n",
        "    obs_dim = env.observation_space.shape[0] if len(\n",
        "        env.observation_space.shape) != 0 else 1\n",
        "    act_dim = env.action_space.shape[0] if len(\n",
        "        env.action_space.shape) != 0 else 1\n",
        "    num_actions = env.action_space.n\n",
        "    lower_act_limit = -1.0\n",
        "    upper_act_limit = 1.0\n",
        "    continuous = False\n",
        "\n",
        "config_net, dir_checkpoints = Utility.get_configuration(args.env_name, args.pomdp_type, args.config_filename)\n",
        "logger = tf.summary.create_file_writer(logdir=dir_checkpoints + \"log_dir/\", experimental_trackable=True,\n",
        "                                        max_queue=10)\n",
        "logger_test = tf.summary.create_file_writer(logdir=dir_checkpoints + \"log_dir/test/\", max_queue=10)\n",
        "\n",
        "# Hyper parameters from arg parsing\n",
        "\n",
        "#num_episodes = args.num_episodes\n",
        "warmup_episodes = args.num_warmup_episodes\n",
        "\n",
        "# Hyper parameters from config file\n",
        "\n",
        "hyper_parameters = config_net[\"hyper_parameters\"]\n",
        "knn_ratio = hyper_parameters[\"knn_ratio\"]\n",
        "buffer_capacity = hyper_parameters[\"replay_buffer_capacity\"]\n",
        "episode_steps = hyper_parameters[\"steps_per_episodes\"]\n",
        "buffer_length = hyper_parameters[\"buffer_length\"]\n",
        "gamma = hyper_parameters[\"discount_factor\"]\n",
        "tau = hyper_parameters[\"target_update_rate\"]\n",
        "lr_critic_parameters = hyper_parameters[\"learning_rate_critic\"]\n",
        "lr_actor_parameters = hyper_parameters[\"learning_rate_actor\"]\n",
        "batch_size = hyper_parameters[\"batch_sizes\"]\n",
        "max_hist_length = hyper_parameters[\"history_length\"]\n",
        "std_dev_act_inf = hyper_parameters[\"std_dev_actor\"]\n",
        "std_dev_act_update = hyper_parameters[\"std_dev_actor_target\"]\n",
        "clip_noise = hyper_parameters[\"clip_noise\"]\n",
        "policy_delay = hyper_parameters[\"policy_delay\"]\n",
        "\n",
        "# Agent initialization\n",
        "actor_critic_main = ActorCriticAgent(obs_dim, config_net, act_dim, continuous, upper_act_limit, lower_act_limit, num_actions,\n",
        "                                      knn_ratio, std_dev_act_inf, clip_noise=None, hist_len=None,\n",
        "                                      agent_type=\"main\")\n",
        "actor_critic_target = ActorCriticAgent(obs_dim, config_net, act_dim, continuous, upper_act_limit, lower_act_limit,\n",
        "                                        num_actions,\n",
        "                                        knn_ratio, std_dev_act_update, clip_noise=clip_noise, hist_len=max_hist_length,\n",
        "                                        agent_type=\"target\")\n",
        "actor_critic_target.set_weights(actor_critic_main.get_weights())\n",
        "\n",
        "# Replay buffer\n",
        "replay_buffer = ReplayBuffer(\n",
        "    obs_dim=obs_dim, act_dim=act_dim, capacity=buffer_capacity, device=device_name)\n",
        "\n",
        "# Parameters optimizers\n",
        "critic_opt = tf.keras.optimizers.Adam(learning_rate=lr_critic_parameters)\n",
        "actor_opt = tf.keras.optimizers.Adam(learning_rate=lr_actor_parameters)\n",
        "\n",
        "start_episode = tf.Variable(0)\n",
        "# Count each time an update occurs, and save this variable in checkpoint in order to correctly restart updating.\n",
        "update_counter = tf.Variable(0)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(main_model=actor_critic_main, target_model=actor_critic_target,\n",
        "                                  rep_buffer=replay_buffer.buffer, critic_optimizer=critic_opt,\n",
        "                                  actor_optimizer=actor_opt, ep_number=start_episode, update_counter=update_counter,\n",
        "                                  logger=logger)\n",
        "\n",
        "checkpoint_manager = tf.train.CheckpointManager(\n",
        "    checkpoint, directory=dir_checkpoints, max_to_keep=1)\n",
        "\n",
        "if checkpoint_manager.latest_checkpoint:\n",
        "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
        "else:\n",
        "    # If no previous experience, then execute warmup episodes in order to store data in Replay buffer\n",
        "    run_warmup_episodes(warmup_episodes)\n",
        "    start_episode.assign(warmup_episodes - 1)\n",
        "\n",
        "start_episode.assign_add(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main cycle in which TD3 algorithm is performend and where agents interact with the environment along with weights' update."
      ],
      "metadata": {
        "id": "s8OCaMGpQ3FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(num_episodes=1000):\n",
        "    with tqdm.trange(start_episode.numpy(), num_episodes) as episodes:\n",
        "            for episode in episodes:\n",
        "                obs = tf.expand_dims(tf.convert_to_tensor(\n",
        "                    env.reset(), dtype=tf.float32), axis=0)\n",
        "                ep_reward = tf.Variable(initial_value=(\n",
        "                    0,), dtype=tf.float32, shape=(1,))\n",
        "                ep_length = 0\n",
        "                loss_critic_values, loss_actor_values = [], []\n",
        "\n",
        "                act_buffer = tf.Variable(1)\n",
        "                # Set initial input to actor model where previous experience has been set to zero\n",
        "                input_act_selection = {\"memory\": {\"obs\": tf.zeros([1, 1, obs_dim], dtype=tf.float32),\n",
        "                                                  \"act\": tf.zeros([1, 1, act_dim], dtype=tf.float32)},\n",
        "                                      \"features\": {\"obs\": obs,\n",
        "                                                    \"act\": None}}\n",
        "\n",
        "                with logger.as_default(step=episode):\n",
        "\n",
        "                    for step in tf.range(episode_steps):\n",
        "\n",
        "                        next_input_act_selection, step_reward, done_step = env_interaction(input_act_selection,\n",
        "                                                                                          actor_critic_main,\n",
        "                                                                                          act_buffer)\n",
        "                        ep_reward.assign_add(step_reward)\n",
        "                        ep_length += 1\n",
        "\n",
        "                        obs = input_act_selection[\"features\"][\"obs\"]\n",
        "                        next_obs = next_input_act_selection[\"features\"][\"obs\"]\n",
        "                        # Get last action, correspondent to the action performed at current step\n",
        "                        act = next_input_act_selection[\"memory\"][\"act\"][0][-1:, :]\n",
        "\n",
        "                        # Force done to one when time horizon has been reached\n",
        "                        done_step = tf.ones_like(\n",
        "                            done_step) if step == episode_steps - 1 else done_step\n",
        "                        replay_buffer.put(obs, act, step_reward,\n",
        "                                          next_obs, done_step)\n",
        "\n",
        "                        input_act_selection = next_input_act_selection\n",
        "                        act_buffer.assign(buffer_length)\n",
        "\n",
        "                        # Update parameters each  step\n",
        "                        update_counter.assign_add(1)\n",
        "                        loss_critic, loss_actor = update(actor_critic_main, actor_critic_target, update_counter,\n",
        "                                                        batch_size, max_hist_length, policy_delay, tau, gamma,\n",
        "                                                        critic_opt, actor_opt)\n",
        "\n",
        "                        loss_critic_values.append(loss_critic.numpy())\n",
        "                        if update_counter.numpy() % policy_delay == 0:\n",
        "                            loss_actor_values.append(loss_actor.numpy())\n",
        "\n",
        "                        if tf.cast(done_step, dtype=tf.bool):\n",
        "                            break\n",
        "\n",
        "                    checkpoint_manager.save()\n",
        "                    checkpoint.ep_number.assign_add(1)\n",
        "                    episodes.set_description(f\"Episode:[{episode}]\")\n",
        "                    episodes.set_postfix(episode_reward=ep_reward.numpy()[\n",
        "                                        0], episode_length=ep_length)\n",
        "                    tf.summary.scalar(name=\"Ep_reward\",\n",
        "                                      data=ep_reward.numpy()[0], step=episode)\n",
        "                    tf.summary.scalar(name=\"Ep_length\",\n",
        "                                      data=ep_length, step=episode)\n",
        "                    tf.summary.scalar(name=\"Loss_critic_mean\", data=statistics.mean(\n",
        "                        loss_critic_values), step=episode)\n",
        "                    tf.summary.scalar(name=\"Loss_actor_mean\", data=statistics.mean(\n",
        "                        loss_actor_values), step=episode)"
      ],
      "metadata": {
        "id": "o6sLzLo1P6E-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the training algorithm\n",
        "run_training()"
      ],
      "metadata": {
        "id": "2NfFEfoTmkQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test agent performance"
      ],
      "metadata": {
        "id": "Yu-CY2xSRucV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(num_test_episodes=10):\n",
        "\n",
        "    # Set standard deviation of main to zero since during test is no more necessary to add exploration noise\n",
        "    actor_critic_main.std_deviation_act_noise = 0\n",
        "    # env.render(mode='human')\n",
        "    with tqdm.trange(0, num_test_episodes) as episodes_test:\n",
        "        for episode_test in episodes_test:\n",
        "            obs = tf.expand_dims(tf.convert_to_tensor(env.reset(), dtype=tf.float32), axis=0)\n",
        "            ep_reward = tf.Variable(initial_value=(0,), dtype=tf.float32, shape=(1,))\n",
        "            ep_length = 0\n",
        "\n",
        "            act_buffer = tf.Variable(1)\n",
        "            # Set initial input to actor model where previous experience has been set to zero\n",
        "            input_act_selection = {\"memory\": {\"obs\": tf.zeros([1, 1, obs_dim], dtype=tf.float32),\n",
        "                                              \"act\": tf.zeros([1, 1, act_dim], dtype=tf.float32)},\n",
        "                                   \"features\": {\"obs\": obs,\n",
        "                                                \"act\": None}}\n",
        "\n",
        "            with logger_test.as_default(step=episode_test):\n",
        "\n",
        "\n",
        "                for step in tf.range(episode_steps):\n",
        "\n",
        "                    next_input_act_selection, step_reward, done_step = env_interaction(input_act_selection,\n",
        "                                                                                       actor_critic_main,\n",
        "                                                                                       act_buffer)\n",
        "                    ep_reward.assign_add(step_reward)\n",
        "                    ep_length += 1\n",
        "\n",
        "                    # Force done to one when time horizon has been reached\n",
        "                    done_step = tf.ones_like(done_step) if step == episode_steps - 1 else done_step\n",
        "\n",
        "                    input_act_selection = next_input_act_selection\n",
        "                    act_buffer.assign(buffer_length)\n",
        "\n",
        "                    if tf.cast(done_step, dtype=tf.bool):\n",
        "                        break\n",
        "\n",
        "                episodes_test.set_description(f\"Test episode:[{episode_test}]\")\n",
        "                episodes_test.set_postfix(episode_reward=ep_reward.numpy()[0], episode_length=ep_length)\n",
        "  "
      ],
      "metadata": {
        "id": "suGaStiikOuN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test agent performance \n",
        "run_test()"
      ],
      "metadata": {
        "id": "PimHKFp5kTeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Images belows show two different learnign curves during training for both continue and discrete action space.\n",
        "In the first case is possible to notice that the agente learn quite good and reach performance similar to the same network proposed in the related paper.\n",
        "\n",
        "Despite the discrete case seems to not performs so well, because appears to be stucked at a sub optimal solution, not improving its learning curve.\n",
        "This could be due to problems related to the action space representation selected along with exploration method that after multiple episodes of training doesn't perform as well as the beginning.\n",
        "\n",
        "For both images X-axis represent the number of episodes and Y-axis represents the reward at each episode. The irregolar shadowed line are the exact reward returned instead the smoothed bold line is the exponential moving average performed by Tensorboard with smooting value 0.99. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XMYFaBGmKX_PyKSsrTigry9v3rkV96Ac\" width=\"900\" height=\"400\"/>\n",
        "\n",
        "> Fig 1. Continue action space.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1F1Mws7gvZMWhiEhiffltZ32TziDnWOvg\" width=\"900\" height=\"400\"/>\n",
        "\n",
        "> Fig 2. Discrete action space. \n"
      ],
      "metadata": {
        "id": "BNbUcnGztcR5"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Al7Jah9RfFj1"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}